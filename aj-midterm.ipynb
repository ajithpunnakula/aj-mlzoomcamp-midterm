{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Load the dataset\n",
    "df_train = pd.read_csv('./house-prices-advanced-regression-techniques/train.csv')\n",
    "\n",
    "# Clean column names by removing spaces and converting to lowercase\n",
    "df_train.columns = df_train.columns.str.replace(' ', '_').str.lower()\n",
    "print(f\"Number of input columns: {len(df_train.columns)}\")\n",
    "\n",
    "# Initialize a list to keep track of all dropped columns\n",
    "dropped_columns = []\n",
    "\n",
    "# Drop columns that are not useful for the model\n",
    "columns_to_drop = ['id']\n",
    "df_train = df_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "dropped_columns.extend(columns_to_drop)\n",
    "\n",
    "# Drop columns with a single value\n",
    "single_value_columns = df_train.columns[df_train.nunique() <= 1].tolist()\n",
    "print(f\"Columns with a single value or all the same: {single_value_columns}\")\n",
    "df_train = df_train.drop(columns=single_value_columns, errors='ignore')\n",
    "dropped_columns.extend(single_value_columns)\n",
    "\n",
    "# Drop columns with a high percentage of missing values\n",
    "high_missing_cols = df_train.columns[df_train.isnull().mean() > 0.5].tolist()\n",
    "print(f\"Columns with a high percentage of missing values: {high_missing_cols}\")\n",
    "df_train = df_train.drop(columns=high_missing_cols, errors='ignore')\n",
    "dropped_columns.extend(high_missing_cols)\n",
    "\n",
    "# Drop columns with a high percentage of zero values\n",
    "high_zero_cols = df_train.columns[(df_train == 0).mean() > 0.5].tolist()\n",
    "print(f\"Columns with a high percentage of zero values: {high_zero_cols}\")\n",
    "df_train = df_train.drop(columns=high_zero_cols, errors='ignore')\n",
    "dropped_columns.extend(high_zero_cols)\n",
    "\n",
    "# Numerical columns and categorical columns\n",
    "numerical_columns = df_train.select_dtypes(include=['number']).columns\n",
    "categorical_columns = df_train.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Fill missing values in numerical columns with the mean\n",
    "df_train[numerical_columns] = df_train[numerical_columns].fillna(df_train[numerical_columns].mean())\n",
    "# Fill missing values in categorical columns with the mode\n",
    "df_train[categorical_columns] = df_train[categorical_columns].fillna(df_train[categorical_columns].mode().iloc[0])\n",
    "\n",
    "# Drop high and low correlated columns\n",
    "correlation_matrix = df_train[numerical_columns].corr().abs()\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "high_corr_cols = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]\n",
    "low_corr_cols = [column for column in upper_tri.columns if all(upper_tri[column] < 0.1)]\n",
    "correlation_based_drop = list(set(high_corr_cols + low_corr_cols))\n",
    "print(f\"Columns to drop due to correlation: {correlation_based_drop}\")\n",
    "df_train = df_train.drop(columns=correlation_based_drop, errors='ignore')\n",
    "dropped_columns.extend(correlation_based_drop)\n",
    "# Print all dropped columns\n",
    "print(f\"All dropped columns: {dropped_columns}\")\n",
    "with open('./outputs/dropped_columns.pkl', 'wb') as f:  \n",
    "    pickle.dump(dropped_columns, f)\n",
    "\n",
    "# Perform one-hot encoding on categorical columns\n",
    "dv = DictVectorizer(sparse=False)\n",
    "df_encoded = dv.fit_transform(df_train[categorical_columns].to_dict(orient='records'))\n",
    "df_encoded = pd.DataFrame(df_encoded, columns=dv.get_feature_names_out())\n",
    "df_train = pd.concat([df_train.drop(columns=categorical_columns, errors='ignore'), df_encoded], axis=1)\n",
    "df_train.columns = df_train.columns.str.replace(' ', '_').str.lower()\n",
    "# Save DictVectorizer for later use\n",
    "joblib.dump(dv, './outputs/dv.joblib')\n",
    "\n",
    "# Plot the distribution of the target variable\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_train['saleprice'], kde=True)\n",
    "plt.title('Distribution of Sale Price')\n",
    "plt.show()\n",
    "\n",
    "# Plot the distribution of the target variable after log transformation\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(np.log1p(df_train['saleprice']), kde=True)\n",
    "plt.title('Distribution of Sale Price after Log Transformation')\n",
    "plt.show()\n",
    "\n",
    "# Apply log transformation to the target variable\n",
    "df_train['saleprice'] = np.log1p(df_train['saleprice'])\n",
    "\n",
    "# save the cleaned dataset\n",
    "df_train.to_csv('./outputs/cleaned_training_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# load the cleaned dataset\n",
    "df_train = pd.read_csv('./outputs/cleaned_training_dataset.csv')\n",
    "# Assume df_* is your DataFrame with 'saleprice' as the target variable\n",
    "X = df_train.drop(columns=['saleprice'])\n",
    "Y = df_train['saleprice']\n",
    "\n",
    "# Split the dataset into training and validation datasets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "def print_best_params_and_score(model_name, grid_search):\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV Mean Squared Error for {model_name}: {grid_search.best_score_}\")\n",
    "\n",
    "# Function to evaluate and store results\n",
    "def evaluate_model(model_name, model, params=None):\n",
    "    if params:\n",
    "        grid = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid.fit(X_train, Y_train)\n",
    "        best_model = grid\n",
    "        print_best_params_and_score(model_name, grid)\n",
    "    else:\n",
    "        model.fit(X_train, Y_train)\n",
    "        best_model = model\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    Y_pred = best_model.predict(X_val)\n",
    "    mse = mean_squared_error(Y_val, Y_pred)\n",
    "    print(f\"Validation Mean Squared Error ({model_name}): {mse}\")\n",
    "    return model_name, mse, best_model\n",
    "\n",
    "# Dictionary to hold results\n",
    "model_performance = {}\n",
    "\n",
    "# Linear Regression\n",
    "name, mse, model = evaluate_model(\"Linear Regression\", LinearRegression())\n",
    "model_performance[name] = (mse, model)\n",
    "\n",
    "# Lasso Regression\n",
    "params_lasso = {'alpha': [0.01, 0.1, 1, 10]}\n",
    "name, mse, model = evaluate_model(\"Lasso Regression\", Lasso(), params_lasso)\n",
    "model_performance[name] = (mse, model)\n",
    "\n",
    "# Ridge Regression\n",
    "params_ridge = {'alpha': [0.01, 0.1, 1, 10]}\n",
    "name, mse, model = evaluate_model(\"Ridge Regression\", Ridge(), params_ridge)\n",
    "model_performance[name] = (mse, model)\n",
    "\n",
    "# Random Forest Regression\n",
    "params_rf = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
    "name, mse, model = evaluate_model(\"Random Forest\", RandomForestRegressor(random_state=42), params_rf)\n",
    "model_performance[name] = (mse, model)\n",
    "\n",
    "# Gradient Boosting Regression\n",
    "params_gb = {'n_estimators': [100, 200, 300], 'max_depth': [3, 4, 5], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "name, mse, model = evaluate_model(\"Gradient Boosting\", GradientBoostingRegressor(random_state=42), params_gb)\n",
    "model_performance[name] = (mse, model)\n",
    "\n",
    "# XGBoost Regression\n",
    "params_xgb = {'n_estimators': [100, 200], 'max_depth': [3, 4, 5], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "name, mse, model = evaluate_model(\"XGBoost\", XGBRegressor(random_state=42, objective='reg:squarederror'), params_xgb)\n",
    "model_performance[name] = (mse, model)\n",
    "\n",
    "# LightGBM Regression\n",
    "params_lgbm = {'n_estimators': [100, 200], 'max_depth': [3, 4, 5], 'learning_rate': [0.01, 0.1, 0.2], 'verbosity': [-1]}\n",
    "name, mse, model = evaluate_model(\"LightGBM\", LGBMRegressor(random_state=42), params_lgbm)\n",
    "model_performance[name] = (mse, model)\n",
    "\n",
    "# Identify the best model based on lowest MSE\n",
    "best_model_name, (best_mse, best_model) = min(model_performance.items(), key=lambda x: x[1][0])\n",
    "print(f\"\\nThe best model is {best_model_name} with a validation MSE of {best_mse}\")\n",
    "\n",
    "# Save the best model using pickle\n",
    "with open(f\"./outputs/best_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(best_model, model_file)\n",
    "print(f\"The best model {best_model_name} has been saved as 'best_model.pkl'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import pickle  \n",
    "import joblib  \n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# Load the test dataset\n",
    "df_test = pd.read_csv('./house-prices-advanced-regression-techniques/test.csv')\n",
    "\n",
    "# Clean column names similarly to how it's done in df_train\n",
    "df_test.columns = df_test.columns.str.replace(' ', '_').str.lower()\n",
    "\n",
    "dropped_columns = None\n",
    "with open('./outputs/dropped_columns.pkl', 'rb') as f:  \n",
    "    dropped_columns = pickle.load(f)  \n",
    "# Drop columns as done in the training data preprocessing\n",
    "df_test = df_test.drop(columns=dropped_columns, errors='ignore')\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_columns_test = df_test.select_dtypes(include=['number']).columns\n",
    "categorical_columns_test = df_test.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "# Fill missing values in test set using the same statistics from training set\n",
    "df_test[numerical_columns_test] = df_test[numerical_columns_test].fillna(df_test[numerical_columns_test].mean())\n",
    "df_test[categorical_columns_test] = df_test[categorical_columns_test].fillna(df_test[categorical_columns_test].mode().iloc[0])\n",
    "\n",
    "# # Load the DictVectorizer fitted earlier\n",
    "dv = joblib.load('./outputs/dv.joblib')\n",
    "\n",
    "# Apply the DictVectorizer to encode categorical features in the test dataset\n",
    "df_test_encoded = dv.transform(df_test[categorical_columns_test].to_dict(orient='records'))\n",
    "df_test_encoded = pd.DataFrame(df_test_encoded, columns=dv.get_feature_names_out())\n",
    "\n",
    "# Combine one-hot encoded features with non-categorical features\n",
    "df_test = pd.concat([df_test.drop(columns=categorical_columns_test, errors='ignore'), df_test_encoded], axis=1)\n",
    "df_test.columns = df_test.columns.str.replace(' ', '_').str.lower()\n",
    "X_test = df_test\n",
    "\n",
    "# Split the dataset into training and validation datasets\n",
    "best_model = None\n",
    "# load the best model\n",
    "with open(\"./outputs/best_model.pkl\", \"rb\") as model_file:\n",
    "    best_model = pickle.load(model_file)\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "Y_test_pred_log = best_model.predict(X_test)\n",
    "# Reverse transform to get predictions in the original target scale  \n",
    "Y_test_pred_original = np.expm1(Y_test_pred_log)\n",
    "print(Y_test_pred_original)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midterm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
